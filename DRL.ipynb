{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3df0353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1f3f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d1d3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset\n",
    "def preprocess_data(df):\n",
    "    # Convert 'Weapon Detected' to binary: 1 for 'Yes', 0 for 'No'\n",
    "    df['Weapon Detected'] = df['Weapon Detected'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "    # Encode 'Timestamp' using a LabelEncoder for simplicity\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['Timestamp'] = label_encoder.fit_transform(df['Timestamp'])\n",
    "    \n",
    "    # Convert the dataframe into numpy array for faster operations\n",
    "    state_data = df[['Timestamp', 'Presence Sensor', 'Persons Recognized', 'Weapon Detected']].values\n",
    "    actions = df['Action'].values\n",
    "    \n",
    "    return state_data, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66597719",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('weekday_normal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0f68320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Presence Sensor</th>\n",
       "      <th>Persons Recognized</th>\n",
       "      <th>Weapon Detected</th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00 AM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:10 AM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:20 AM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:30 AM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:40 AM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>11:10 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>11:20 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>11:30 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>11:40 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>11:50 PM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Timestamp  Presence Sensor  Persons Recognized Weapon Detected  Action\n",
       "0    00:00 AM                0                   0              No       0\n",
       "1    00:10 AM                0                   0              No       0\n",
       "2    00:20 AM                0                   0              No       0\n",
       "3    00:30 AM                0                   0              No       0\n",
       "4    00:40 AM                0                   0              No       0\n",
       "..        ...              ...                 ...             ...     ...\n",
       "139  11:10 PM                0                   0              No       0\n",
       "140  11:20 PM                0                   0              No       0\n",
       "141  11:30 PM                0                   0              No       0\n",
       "142  11:40 PM                0                   0              No       0\n",
       "143  11:50 PM                0                   0              No       0\n",
       "\n",
       "[144 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e085319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "state_data, actions = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ce28511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0],\n",
       "       [  1,   0,   0,   0],\n",
       "       [  2,   0,   0,   0],\n",
       "       [  3,   0,   0,   0],\n",
       "       [  4,   0,   0,   0],\n",
       "       [  5,   0,   0,   0],\n",
       "       [  6,   0,   0,   0],\n",
       "       [  8,   0,   0,   0],\n",
       "       [ 10,   0,   0,   0],\n",
       "       [ 12,   0,   0,   0],\n",
       "       [ 14,   0,   0,   0],\n",
       "       [ 16,   0,   0,   0],\n",
       "       [ 18,   0,   0,   0],\n",
       "       [ 20,   0,   0,   0],\n",
       "       [ 22,   0,   0,   0],\n",
       "       [ 24,   0,   0,   0],\n",
       "       [ 26,   0,   0,   0],\n",
       "       [ 28,   0,   0,   0],\n",
       "       [ 30,   0,   0,   0],\n",
       "       [ 32,   0,   0,   0],\n",
       "       [ 34,   0,   0,   0],\n",
       "       [ 36,   0,   0,   0],\n",
       "       [ 38,   0,   0,   0],\n",
       "       [ 40,   0,   0,   0],\n",
       "       [ 42,   0,   0,   0],\n",
       "       [ 44,   0,   0,   0],\n",
       "       [ 46,   0,   0,   0],\n",
       "       [ 48,   0,   0,   0],\n",
       "       [ 50,   0,   0,   0],\n",
       "       [ 52,   0,   0,   0],\n",
       "       [ 54,   0,   0,   0],\n",
       "       [ 56,   0,   0,   0],\n",
       "       [ 58,   0,   0,   0],\n",
       "       [ 60,   1,   1,   0],\n",
       "       [ 62,   1,   1,   0],\n",
       "       [ 64,   1,   2,   0],\n",
       "       [ 66,   1,   2,   0],\n",
       "       [ 68,   1,   4,   0],\n",
       "       [ 70,   1,   4,   0],\n",
       "       [ 72,   1,   4,   0],\n",
       "       [ 74,   1,   4,   0],\n",
       "       [ 76,   1,   4,   0],\n",
       "       [ 78,   1,   4,   0],\n",
       "       [ 80,   1,   4,   0],\n",
       "       [ 82,   1,   4,   0],\n",
       "       [ 84,   1,   4,   0],\n",
       "       [ 86,   1,   4,   0],\n",
       "       [ 88,   1,   4,   0],\n",
       "       [ 90,   1,   4,   0],\n",
       "       [ 92,   0,   0,   0],\n",
       "       [ 94,   0,   0,   0],\n",
       "       [ 96,   0,   0,   0],\n",
       "       [ 98,   0,   0,   0],\n",
       "       [100,   0,   0,   0],\n",
       "       [102,   0,   0,   0],\n",
       "       [104,   0,   0,   0],\n",
       "       [106,   0,   0,   0],\n",
       "       [108,   0,   0,   0],\n",
       "       [110,   0,   0,   0],\n",
       "       [112,   0,   0,   0],\n",
       "       [114,   0,   0,   0],\n",
       "       [116,   0,   0,   0],\n",
       "       [118,   0,   0,   0],\n",
       "       [120,   0,   0,   0],\n",
       "       [122,   0,   0,   0],\n",
       "       [124,   0,   0,   0],\n",
       "       [126,   0,   0,   0],\n",
       "       [128,   0,   0,   0],\n",
       "       [130,   0,   0,   0],\n",
       "       [132,   0,   0,   0],\n",
       "       [134,   0,   0,   0],\n",
       "       [136,   0,   0,   0],\n",
       "       [138,   0,   0,   0],\n",
       "       [139,   0,   0,   0],\n",
       "       [140,   0,   0,   0],\n",
       "       [141,   0,   0,   0],\n",
       "       [142,   0,   0,   0],\n",
       "       [143,   0,   0,   0],\n",
       "       [  7,   0,   0,   0],\n",
       "       [  9,   0,   0,   0],\n",
       "       [ 11,   0,   0,   0],\n",
       "       [ 13,   0,   0,   0],\n",
       "       [ 15,   0,   0,   0],\n",
       "       [ 17,   0,   0,   0],\n",
       "       [ 19,   0,   0,   0],\n",
       "       [ 21,   0,   0,   0],\n",
       "       [ 23,   0,   0,   0],\n",
       "       [ 25,   0,   0,   0],\n",
       "       [ 27,   0,   0,   0],\n",
       "       [ 29,   0,   0,   0],\n",
       "       [ 31,   0,   0,   0],\n",
       "       [ 33,   0,   0,   0],\n",
       "       [ 35,   0,   0,   0],\n",
       "       [ 37,   1,   2,   0],\n",
       "       [ 39,   1,   2,   0],\n",
       "       [ 41,   1,   2,   0],\n",
       "       [ 43,   1,   2,   0],\n",
       "       [ 45,   1,   2,   0],\n",
       "       [ 47,   1,   2,   0],\n",
       "       [ 49,   1,   2,   0],\n",
       "       [ 51,   1,   2,   0],\n",
       "       [ 53,   1,   2,   0],\n",
       "       [ 55,   1,   2,   0],\n",
       "       [ 57,   1,   2,   0],\n",
       "       [ 59,   1,   2,   0],\n",
       "       [ 61,   1,   2,   0],\n",
       "       [ 63,   1,   1,   0],\n",
       "       [ 65,   1,   1,   0],\n",
       "       [ 67,   1,   4,   0],\n",
       "       [ 69,   1,   4,   0],\n",
       "       [ 71,   1,   4,   0],\n",
       "       [ 73,   1,   4,   0],\n",
       "       [ 75,   1,   4,   0],\n",
       "       [ 77,   1,   4,   0],\n",
       "       [ 79,   1,   4,   0],\n",
       "       [ 81,   1,   4,   0],\n",
       "       [ 83,   1,   4,   0],\n",
       "       [ 85,   1,   4,   0],\n",
       "       [ 87,   1,   4,   0],\n",
       "       [ 89,   1,   4,   0],\n",
       "       [ 91,   1,   4,   0],\n",
       "       [ 93,   1,   4,   0],\n",
       "       [ 95,   1,   4,   0],\n",
       "       [ 97,   1,   4,   0],\n",
       "       [ 99,   1,   4,   0],\n",
       "       [101,   1,   4,   0],\n",
       "       [103,   1,   4,   0],\n",
       "       [105,   1,   4,   0],\n",
       "       [107,   1,   4,   0],\n",
       "       [109,   1,   4,   0],\n",
       "       [111,   1,   4,   0],\n",
       "       [113,   1,   4,   0],\n",
       "       [115,   1,   2,   0],\n",
       "       [117,   1,   2,   0],\n",
       "       [119,   0,   0,   0],\n",
       "       [121,   0,   0,   0],\n",
       "       [123,   0,   0,   0],\n",
       "       [125,   0,   0,   0],\n",
       "       [127,   0,   0,   0],\n",
       "       [129,   0,   0,   0],\n",
       "       [131,   0,   0,   0],\n",
       "       [133,   0,   0,   0],\n",
       "       [135,   0,   0,   0],\n",
       "       [137,   0,   0,   0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9801c357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4b88c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom environment\n",
    "class SecuritySystemEnv(gym.Env):\n",
    "    def __init__(self, state_data, actions):\n",
    "        super(SecuritySystemEnv, self).__init__()\n",
    "        self.state_data = state_data  # The states from the dataset\n",
    "        self.actions_data = actions  # The action column\n",
    "        \n",
    "        # The state is defined by the 4 columns: Timestamp, Presence Sensor, Persons Recognized, Weapon Detected\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(4,), dtype=np.float32)\n",
    "        \n",
    "        # The agent can take 2 actions: 0 (do nothing) or 2 (inform owner and police)\n",
    "        self.action_space = spaces.Discrete(2)  # Actions: 0 or 2\n",
    "        \n",
    "        # Track the current index (row in the dataset)\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the environment to the first state\n",
    "        self.current_step = 0\n",
    "        return self.state_data[self.current_step]\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Get the correct action from the dataset for this step\n",
    "        correct_action = self.actions_data[self.current_step]\n",
    "\n",
    "        # Calculate reward: +1 if action matches the dataset action, -1 otherwise\n",
    "        reward = 1 if (action * 2) == correct_action else -1\n",
    "\n",
    "        # Move to the next step (next row in the dataset)\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Check if we have reached the end of the dataset\n",
    "        done = self.current_step >= len(self.state_data)\n",
    "        #done = self.current_step < len(self.state_data)\n",
    "\n",
    "        # Get the next state\n",
    "        if not done:\n",
    "            next_state = self.state_data[self.current_step]\n",
    "        else:\n",
    "            next_state = np.zeros(self.state_data.shape[1])  # Return a dummy state at the end\n",
    "            self.current_step = len(self.state_data) - 1  # Ensure current_step doesn't go out of bounds\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step: {self.current_step}, State: {self.state_data[self.current_step]}\")\n",
    "        #print(f\"Step: {self.current_step}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a79200d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8878528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = SecuritySystemEnv(state_data, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "614c9f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9fd45fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([137,   0,   0,   0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_data[143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a3961114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DQN model\n",
    "model = DQN('MlpPolicy', env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f99a0043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 32.5     |\n",
      "|    exploration_rate | 0.453    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 2240     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 576      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0929   |\n",
      "|    n_updates        | 118      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 75       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1899     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1152     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.106    |\n",
      "|    n_updates        | 262      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 94.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 1773     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1728     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0769   |\n",
      "|    n_updates        | 406      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 105      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1731     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 2304     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0264   |\n",
      "|    n_updates        | 550      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 111      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 1691     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 2880     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0398   |\n",
      "|    n_updates        | 694      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 115      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1668     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 3456     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0123   |\n",
      "|    n_updates        | 838      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 119      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1660     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 4032     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00148  |\n",
      "|    n_updates        | 982      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 121      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 1648     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 4608     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00865  |\n",
      "|    n_updates        | 1126     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 123      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 1638     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 5184     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00504  |\n",
      "|    n_updates        | 1270     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 124      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 1637     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 5760     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00122  |\n",
      "|    n_updates        | 1414     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 125      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 1634     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 6336     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00111  |\n",
      "|    n_updates        | 1558     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 126      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 1634     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 6912     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00132  |\n",
      "|    n_updates        | 1702     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 127      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 1636     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 7488     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00228  |\n",
      "|    n_updates        | 1846     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 128      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 1637     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 8064     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000943 |\n",
      "|    n_updates        | 1990     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 128      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 1635     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 8640     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0006   |\n",
      "|    n_updates        | 2134     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 129      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 1633     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 9216     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000309 |\n",
      "|    n_updates        | 2278     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | 129      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 1631     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 9792     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000182 |\n",
      "|    n_updates        | 2422     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f47494e8b20>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3decad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save(\"dqn_security_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "760c09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model (if needed in the future)\n",
    "# model = DQN.load(\"dqn_security_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c30ecebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "70d5c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward is 1\n",
      "Step: 1, State: [1 0 0 0]\n",
      "Reward is 1\n",
      "Step: 2, State: [2 0 0 0]\n",
      "Reward is 1\n",
      "Step: 3, State: [3 0 0 0]\n",
      "Reward is 1\n",
      "Step: 4, State: [4 0 0 0]\n",
      "Reward is 1\n",
      "Step: 5, State: [5 0 0 0]\n",
      "Reward is 1\n",
      "Step: 6, State: [6 0 0 0]\n",
      "Reward is 1\n",
      "Step: 7, State: [8 0 0 0]\n",
      "Reward is 1\n",
      "Step: 8, State: [10  0  0  0]\n",
      "Reward is 1\n",
      "Step: 9, State: [12  0  0  0]\n",
      "Reward is 1\n",
      "Step: 10, State: [14  0  0  0]\n",
      "Reward is 1\n",
      "Step: 11, State: [16  0  0  0]\n",
      "Reward is 1\n",
      "Step: 12, State: [18  0  0  0]\n",
      "Reward is 1\n",
      "Step: 13, State: [20  0  0  0]\n",
      "Reward is 1\n",
      "Step: 14, State: [22  0  0  0]\n",
      "Reward is 1\n",
      "Step: 15, State: [24  0  0  0]\n",
      "Reward is 1\n",
      "Step: 16, State: [26  0  0  0]\n",
      "Reward is 1\n",
      "Step: 17, State: [28  0  0  0]\n",
      "Reward is 1\n",
      "Step: 18, State: [30  0  0  0]\n",
      "Reward is 1\n",
      "Step: 19, State: [32  0  0  0]\n",
      "Reward is 1\n",
      "Step: 20, State: [34  0  0  0]\n",
      "Reward is 1\n",
      "Step: 21, State: [36  0  0  0]\n",
      "Reward is 1\n",
      "Step: 22, State: [38  0  0  0]\n",
      "Reward is 1\n",
      "Step: 23, State: [40  0  0  0]\n",
      "Reward is 1\n",
      "Step: 24, State: [42  0  0  0]\n",
      "Reward is 1\n",
      "Step: 25, State: [44  0  0  0]\n",
      "Reward is 1\n",
      "Step: 26, State: [46  0  0  0]\n",
      "Reward is 1\n",
      "Step: 27, State: [48  0  0  0]\n",
      "Reward is 1\n",
      "Step: 28, State: [50  0  0  0]\n",
      "Reward is 1\n",
      "Step: 29, State: [52  0  0  0]\n",
      "Reward is 1\n",
      "Step: 30, State: [54  0  0  0]\n",
      "Reward is 1\n",
      "Step: 31, State: [56  0  0  0]\n",
      "Reward is 1\n",
      "Step: 32, State: [58  0  0  0]\n",
      "Reward is 1\n",
      "Step: 33, State: [60  1  1  0]\n",
      "Reward is 1\n",
      "Step: 34, State: [62  1  1  0]\n",
      "Reward is 1\n",
      "Step: 35, State: [64  1  2  0]\n",
      "Reward is 1\n",
      "Step: 36, State: [66  1  2  0]\n",
      "Reward is 1\n",
      "Step: 37, State: [68  1  4  0]\n",
      "Reward is 1\n",
      "Step: 38, State: [70  1  4  0]\n",
      "Reward is 1\n",
      "Step: 39, State: [72  1  4  0]\n",
      "Reward is 1\n",
      "Step: 40, State: [74  1  4  0]\n",
      "Reward is 1\n",
      "Step: 41, State: [76  1  4  0]\n",
      "Reward is 1\n",
      "Step: 42, State: [78  1  4  0]\n",
      "Reward is 1\n",
      "Step: 43, State: [80  1  4  0]\n",
      "Reward is 1\n",
      "Step: 44, State: [82  1  4  0]\n",
      "Reward is -1\n",
      "Step: 45, State: [84  1  4  0]\n",
      "Reward is 1\n",
      "Step: 46, State: [86  1  4  0]\n",
      "Reward is 1\n",
      "Step: 47, State: [88  1  4  0]\n",
      "Reward is 1\n",
      "Step: 48, State: [90  1  4  0]\n",
      "Reward is 1\n",
      "Step: 49, State: [92  0  0  0]\n",
      "Reward is 1\n",
      "Step: 50, State: [94  0  0  0]\n",
      "Reward is 1\n",
      "Step: 51, State: [96  0  0  0]\n",
      "Reward is 1\n",
      "Step: 52, State: [98  0  0  0]\n",
      "Reward is 1\n",
      "Step: 53, State: [100   0   0   0]\n",
      "Reward is 1\n",
      "Step: 54, State: [102   0   0   0]\n",
      "Reward is 1\n",
      "Step: 55, State: [104   0   0   0]\n",
      "Reward is 1\n",
      "Step: 56, State: [106   0   0   0]\n",
      "Reward is 1\n",
      "Step: 57, State: [108   0   0   0]\n",
      "Reward is 1\n",
      "Step: 58, State: [110   0   0   0]\n",
      "Reward is 1\n",
      "Step: 59, State: [112   0   0   0]\n",
      "Reward is -1\n",
      "Step: 60, State: [114   0   0   0]\n",
      "Reward is 1\n",
      "Step: 61, State: [116   0   0   0]\n",
      "Reward is 1\n",
      "Step: 62, State: [118   0   0   0]\n",
      "Reward is 1\n",
      "Step: 63, State: [120   0   0   0]\n",
      "Reward is 1\n",
      "Step: 64, State: [122   0   0   0]\n",
      "Reward is 1\n",
      "Step: 65, State: [124   0   0   0]\n",
      "Reward is 1\n",
      "Step: 66, State: [126   0   0   0]\n",
      "Reward is 1\n",
      "Step: 67, State: [128   0   0   0]\n",
      "Reward is 1\n",
      "Step: 68, State: [130   0   0   0]\n",
      "Reward is 1\n",
      "Step: 69, State: [132   0   0   0]\n",
      "Reward is 1\n",
      "Step: 70, State: [134   0   0   0]\n",
      "Reward is 1\n",
      "Step: 71, State: [136   0   0   0]\n",
      "Reward is 1\n",
      "Step: 72, State: [138   0   0   0]\n",
      "Reward is 1\n",
      "Step: 73, State: [139   0   0   0]\n",
      "Reward is 1\n",
      "Step: 74, State: [140   0   0   0]\n",
      "Reward is 1\n",
      "Step: 75, State: [141   0   0   0]\n",
      "Reward is 1\n",
      "Step: 76, State: [142   0   0   0]\n",
      "Reward is 1\n",
      "Step: 77, State: [143   0   0   0]\n",
      "Reward is 1\n",
      "Step: 78, State: [7 0 0 0]\n",
      "Reward is 1\n",
      "Step: 79, State: [9 0 0 0]\n",
      "Reward is 1\n",
      "Step: 80, State: [11  0  0  0]\n",
      "Reward is 1\n",
      "Step: 81, State: [13  0  0  0]\n",
      "Reward is 1\n",
      "Step: 82, State: [15  0  0  0]\n",
      "Reward is 1\n",
      "Step: 83, State: [17  0  0  0]\n",
      "Reward is 1\n",
      "Step: 84, State: [19  0  0  0]\n",
      "Reward is 1\n",
      "Step: 85, State: [21  0  0  0]\n",
      "Reward is 1\n",
      "Step: 86, State: [23  0  0  0]\n",
      "Reward is 1\n",
      "Step: 87, State: [25  0  0  0]\n",
      "Reward is 1\n",
      "Step: 88, State: [27  0  0  0]\n",
      "Reward is 1\n",
      "Step: 89, State: [29  0  0  0]\n",
      "Reward is 1\n",
      "Step: 90, State: [31  0  0  0]\n",
      "Reward is 1\n",
      "Step: 91, State: [33  0  0  0]\n",
      "Reward is 1\n",
      "Step: 92, State: [35  0  0  0]\n",
      "Reward is 1\n",
      "Step: 93, State: [37  1  2  0]\n",
      "Reward is 1\n",
      "Step: 94, State: [39  1  2  0]\n",
      "Reward is 1\n",
      "Step: 95, State: [41  1  2  0]\n",
      "Reward is 1\n",
      "Step: 96, State: [43  1  2  0]\n",
      "Reward is 1\n",
      "Step: 97, State: [45  1  2  0]\n",
      "Reward is 1\n",
      "Step: 98, State: [47  1  2  0]\n",
      "Reward is 1\n",
      "Step: 99, State: [49  1  2  0]\n",
      "Reward is -1\n",
      "Step: 100, State: [51  1  2  0]\n",
      "Reward is 1\n",
      "Step: 101, State: [53  1  2  0]\n",
      "Reward is 1\n",
      "Step: 102, State: [55  1  2  0]\n",
      "Reward is 1\n",
      "Step: 103, State: [57  1  2  0]\n",
      "Reward is -1\n",
      "Step: 104, State: [59  1  2  0]\n",
      "Reward is 1\n",
      "Step: 105, State: [61  1  2  0]\n",
      "Reward is 1\n",
      "Step: 106, State: [63  1  1  0]\n",
      "Reward is 1\n",
      "Step: 107, State: [65  1  1  0]\n",
      "Reward is 1\n",
      "Step: 108, State: [67  1  4  0]\n",
      "Reward is 1\n",
      "Step: 109, State: [69  1  4  0]\n",
      "Reward is 1\n",
      "Step: 110, State: [71  1  4  0]\n",
      "Reward is 1\n",
      "Step: 111, State: [73  1  4  0]\n",
      "Reward is 1\n",
      "Step: 112, State: [75  1  4  0]\n",
      "Reward is 1\n",
      "Step: 113, State: [77  1  4  0]\n",
      "Reward is 1\n",
      "Step: 114, State: [79  1  4  0]\n",
      "Reward is 1\n",
      "Step: 115, State: [81  1  4  0]\n",
      "Reward is 1\n",
      "Step: 116, State: [83  1  4  0]\n",
      "Reward is 1\n",
      "Step: 117, State: [85  1  4  0]\n",
      "Reward is 1\n",
      "Step: 118, State: [87  1  4  0]\n",
      "Reward is 1\n",
      "Step: 119, State: [89  1  4  0]\n",
      "Reward is 1\n",
      "Step: 120, State: [91  1  4  0]\n",
      "Reward is 1\n",
      "Step: 121, State: [93  1  4  0]\n",
      "Reward is 1\n",
      "Step: 122, State: [95  1  4  0]\n",
      "Reward is 1\n",
      "Step: 123, State: [97  1  4  0]\n",
      "Reward is 1\n",
      "Step: 124, State: [99  1  4  0]\n",
      "Reward is 1\n",
      "Step: 125, State: [101   1   4   0]\n",
      "Reward is 1\n",
      "Step: 126, State: [103   1   4   0]\n",
      "Reward is 1\n",
      "Step: 127, State: [105   1   4   0]\n",
      "Reward is 1\n",
      "Step: 128, State: [107   1   4   0]\n",
      "Reward is 1\n",
      "Step: 129, State: [109   1   4   0]\n",
      "Reward is 1\n",
      "Step: 130, State: [111   1   4   0]\n",
      "Reward is 1\n",
      "Step: 131, State: [113   1   4   0]\n",
      "Reward is 1\n",
      "Step: 132, State: [115   1   2   0]\n",
      "Reward is 1\n",
      "Step: 133, State: [117   1   2   0]\n",
      "Reward is 1\n",
      "Step: 134, State: [119   0   0   0]\n",
      "Reward is 1\n",
      "Step: 135, State: [121   0   0   0]\n",
      "Reward is 1\n",
      "Step: 136, State: [123   0   0   0]\n",
      "Reward is 1\n",
      "Step: 137, State: [125   0   0   0]\n",
      "Reward is 1\n",
      "Step: 138, State: [127   0   0   0]\n",
      "Reward is -1\n",
      "Step: 139, State: [129   0   0   0]\n",
      "Reward is 1\n",
      "Step: 140, State: [131   0   0   0]\n",
      "Reward is 1\n",
      "Step: 141, State: [133   0   0   0]\n",
      "Reward is 1\n",
      "Step: 142, State: [135   0   0   0]\n",
      "Reward is 1\n",
      "Step: 143, State: [137   0   0   0]\n",
      "Reward is 1\n",
      "Step: 143, State: [137   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "while not done:\n",
    "    action, _ = model.predict(state)  # Predict the action from the trained model\n",
    "    next_state, reward, done, _ = env.step(action)  # Take the action\n",
    "    print(f\"Reward is {reward}\")\n",
    "    total_reward += reward  # Accumulate rewards\n",
    "    env.render()  # Print the current state\n",
    "    state = next_state  # Move to the next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f03ce39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34648a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
